---
title: "data.data"
output:
  html_document:
    df_print: paged
---

# Library

```{r}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(readr)
library(survival)
library(survminer)
library(randomForestSRC)
library(gbm)
library(utils) #For not having to upload the txt each time


```

# Importing Data

```{r}

#setwd('~/Desktop/Data Science/Data Science Lab in Medicine/Big Data in Health Care/')
url <- "https://raw.githubusercontent.com/darthgween/BigDataInHealthCareProject/main/heart.valve.txt?token=GHSAT0AAAAAACR3GYSXO5Z3ARKVQY2S6SYSZRYZQWA"
data <- read.table(url, header = TRUE)
head(data)
```

```{r}

#TO EVALUATE
# data$sex <- factor(data$sex, levels = c(0, 1), labels = c("Male", "Female"))
# data$con.cabg <- factor(data$con.cabg, levels = c(0, 1), labels = c("No", "Yes"))
# data$lv <- factor(data$lv, levels = c(1, 2, 3), labels = c("High", "Moderate", "Low"))
# data$sten.reg.mix <- factor(data$sten.reg.mix, levels = c(1, 2, 3), labels = c("Stenosis", "Regurgitation", "Mixed"))

```


```{r}
dim(data)
```

Description of dataset variables:

-   ***Paz.id***: patient identification number.

-   ***log.lvmi***: natural logarithm of the “Left Ventricular Mass Index” (standardized) measured at baseline.

-   ***fuyrs***: follow-up time from surgery, in years.

-   ***status***: event indicator (1 = dead; 0 = censored).

-   ***sex***: sex of the patient (0 = M; 1 = F).

-   ***age***: age of the patient (years) at surgery.

-   ***con.cabg***: presence of coronary bypass (1 = yes; 0 = no).

-   ***creat***: pre-operative serum creatinine (µmol/mL).

-   ***lv***: pre-operative left ventricular ejection fraction (1 = high, 2 = moderate, 3 = low).

-   ***sten.reg.mix***: hemodynamics of the heart valve (1 = stenosis, 2 = regurgitation, 3 = mixed).

```{r}
str(data)
```

```{r}
summary(data)
```

# Age distribution

```{r}
ggplot(data, aes(x = age)) +
  geom_histogram(binwidth = 5, fill = "blue", color = "black") +
  ggtitle("Distribuzione dell'età") +
  xlab("Età") +
  ylab("Frequenza")
```

#Correlation between variables

```{r}
cor_matrix <- cor(data, use = "complete.obs")
print(cor_matrix)

if (!require(corrplot)) {
    install.packages("corrplot")
    library(corrplot)
}
corrplot(cor_matrix, method = "circle")
```


## Left Ventricular Mass Index

Il Left Ventricular Mass Index (LVMI) è un parametro clinico utilizzato per valutare la massa del ventricolo sinistro del cuore in relazione alla superficie corporea del paziente. È una misura importante nella diagnosi e nel monitoraggio di condizioni cardiache come l'ipertrofia ventricolare sinistra, che è un ispessimento delle pareti del ventricolo sinistro del cuore.

```{r}
ggplot(data, aes(x = factor(status), y = log.lvmi, fill = factor(status))) +
  geom_boxplot() +
  ggtitle("Boxplot di log.lvmi per Status") +
  xlab("Status") +
  ylab("log.lvmi") +
  scale_fill_discrete(name = "Status")

```

```{r}
ggplot(data, aes(x = age, y = log.lvmi)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  ggtitle("Scatter Plot: età vs log.lvmi") +
  xlab("Età") +
  ylab("log.lvmi")

```

# Models:

```{r}
univariate_models <- lapply(names(data)[-1], function(var) {
  cox_model <- coxph(Surv(fuyrs, status) ~ data[[var]], data = data)
  return(list(variable = var, model = cox_model))
})

```

```{r}
for (item in univariate_models) {
  print(item$variable)
  print(summary(item$model))
  
}

```

```{r}
  print(univariate_models[[1]]$variable)
  print(summary(univariate_models[[1]]$model))
```

```{r}
  print(univariate_models[[2]]$variable)
  print(summary(univariate_models[[2]]$model))
```


```{r}
  print(univariate_models[[3]]$variable)
  print(summary(univariate_models[[3]]$model))
```


```{r}
  print(univariate_models[[4]]$variable)
  print(summary(univariate_models[[4]]$model))
```


```{r}
  print(univariate_models[[5]]$variable)
  print(summary(univariate_models[[5]]$model))
```


```{r}
  print(univariate_models[[6]]$variable)
  print(summary(univariate_models[[6]]$model))
```


```{r}
  print(univariate_models[[7]]$variable)
  print(summary(univariate_models[[7]]$model))
```


```{r}
  print(univariate_models[[8]]$variable)
  print(summary(univariate_models[[8]]$model))
```


```{r}
  print(univariate_models[[9]]$variable)
  print(summary(univariate_models[[9]]$model))
```

## Base model

```{r}
base_model <- coxph(Surv(fuyrs, status) ~ sex + age + con.cabg + creat + lv + sten.reg.mix, data = data)
summary(base_model)


fit1<-survfit(base_model,newdata=data)
data$risk_base<-1-as.numeric(summary(fit1,times=5)$surv)

```
The Cox proportional hazards model shows significant effects of age, coronary artery bypass grafting (con.cabg), left ventricular function (lv), and stenosis severity (sten.reg.mix) on survival time. 

Sex and creatinine levels (creat) do not appear to significantly affect survival. 

The model has moderate predictive accuracy (Concordance = 0.783) and overall good fit according to model fit tests. 


```{r}
# Evaluate the performance of the models
# For calibration, discrimination, and Net Benefit, you can use specific methods like Kaplan-Meier curves, calibration plots, concordance index (C-index), and decision curve analysis (DCA).

# Example of Kaplan-Meier curves for the base model
ggsurvplot(survfit(base_model), data = data, risk.table = TRUE)

```

```{r}
# Load necessary libraries
#install.packages("rms")
library(rms)
#install.packages("survival")
library(survival)

# Assuming data is your dataframe with the relevant variables
# Example dataset
# data <- data.frame(fuyrs = ..., status = ..., sex = ..., age = ..., con.cabg = ..., creat = ..., lv = ..., sten.reg.mix = ...)

# Set up the data distribution for rms package
dd <- datadist(heart.valve)
options(datadist = 'dd')

# Fit the Cox proportional hazards model using cph from rms package
cph_model <- cph(Surv(fuyrs, status) ~ sex + age + con.cabg + creat + lv + sten.reg.mix, data = data, x = TRUE, y = TRUE, surv = TRUE)

# Calibrate the model
calib <- calibrate(cph_model, method = "boot", B = 200, u = 5)

# Plot the calibration curve
plot(calib, xlim = c(0, 1), ylim = c(0, 1), xlab = "Predicted Probability", ylab = "Observed Probability", main = "Calibration Plot")

```

```{r}
# TRYING CALIBRATION PLOTS
library(survival)
library(rms)
library(ggplot2)
install.packages('survAUC')
library(survAUC)
library(rms)

# Create a calibration plot
calib <- calibrate(base_model, method = "boot", B = 100)
calib_plot <- ggplot(calib, aes(x = .score, y = .cal)) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(x = "Predicted Risk", y = "Observed Risk", title = "Calibration Plot")

# Print the calibration plot
print(calib_plot)

# Calculate the Brier score
brier_score <- brier.score(Surv(time, status) ~ predict(base_model), data = data, method = "KM")
print(paste("Brier Score: ", brier_score))
```


```{r}
fit<-survfit(Surv(fuyrs, status) ~ 1, data)
plot(fit, xlab='Years since transplant', ylab='Survival probability')
```





```{r}
#### Test Proportional Hazards - Martingale and Schoenfeld

# Function to plot Martingale residuals for continuous variables - FUNCTIONAL FORM
plot_martingale <- function(variable) {
  # Compute residuals
  martingale_res <- residuals(base_model, type = "martingale")
  
  # Create a data frame for plotting
  plot_data <- data.frame(Variable = data[[variable]], Residuals = martingale_res)
  
  # Plot
  ggplot(plot_data, aes(x = Variable, y = Residuals)) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "loess", color = "red") +
    labs(title = paste("Martingale residuals vs", variable),
         x = variable,
         y = "Martingale Residuals")
}

# Plot Martingale residuals for 'age' and 'creat'- FUNCTIONAL FORM
plot_martingale("age")
plot_martingale("creat")

# Function to check proportional hazards using Schoenfeld residuals - PH ASSUMPTION

# Plot the residuals
plot(cox.zph(base_model))



```

```{r}
# Prediction of event risk at a fixed time-point (e.g., 5 years) for three subjects
# You can predict the survival probability for specific subjects using the predict function.

predict(base_model, newdata = data.frame(sex = c(0, 1, 0), age = c(50, 60, 70), con.cabg = c(1, 0, 1), creat = c(1.2, 1.5, 1.3), lv = c(2, 3, 1), sten.reg.mix = c(1, 3, 2), log.lvmi = c(2.5, 3.0, 2.7)), type = "risk", times = 5)
```

## Augmented model

```{r}
augmented_model <- coxph(Surv(fuyrs, status) ~ sex + age + con.cabg + creat + lv + sten.reg.mix + log.lvmi, data = data)

summary(augmented_model)

fit2<-survfit(augmented_model,newdata=data)
data$risk_augmented<-1-as.numeric(summary(fit2,times=5)$surv)

```


The augmented model includes log.lvmi as a new predictor, significantly improving predictive accuracy and model fit compared to the base model. Notably, con.cabg and sten.reg.mix show stronger effects, while log.lvmi emerges as a significant predictor. The concordance value (0.797) is slightly higher than the base_model so we can conclude that the augmented one with the new covariate log.lvmi is better than the base model






```{r}
# Evaluate functional form of continuous variables using Martingale residuals

# Function to plot Martingale residuals for continuous variables - FUNCTIONAL FORM
plot_martingale_aug <- function(variable) {
  # Compute residuals
  martingale_res <- residuals(augmented_model, type = "martingale")
  
  # Create a data frame for plotting
  plot_data <- data.frame(Variable = data[[variable]], Residuals = martingale_res)
  
  # Plot
  ggplot(plot_data, aes(x = Variable, y = Residuals)) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "loess", color = "red") +
    labs(title = paste("Martingale residuals vs", variable),
         x = variable,
         y = "Martingale Residuals")
}

# Plot Martingale residuals for 'age', 'creat' and 'log.lvmi' - FUNCTIONAL FORM
plot_martingale_aug("age")
plot_martingale_aug("creat")
plot_martingale_aug("log.lvmi")

```

```{r}
# Check the proportional hazards assumption for the augmented model
cox.zph(augmented_model)


# Plot the Schoenfeld residuals for the augmented model
plot(cox.zph(augmented_model))
```

```{r}

# Example of Kaplan-Meier curves for the augmented model
ggsurvplot(survfit(augmented_model), data = data, risk.table = TRUE)

```

```{r}

# Predictions for augmented model
predict(augmented_model, newdata = data.frame(sex = c(0, 1, 0), age = c(50, 60, 80), con.cabg = c(1, 0, 1), creat = c(1.2, 1.5, 1.3), lv = c(2, 3, 1), sten.reg.mix = c(1, 3, 2), log.lvmi = c(2.5, 3.0, 2.7)), type = "risk", times = 5)
```

## Random Survival Forest model
```{r}
library(randomForestSRC)

# Fit Random Survival Forest model
rsf_model <- rfsrc(Surv(fuyrs, status) ~ ., data = data)

# Summary of the Random Survival Forest model
print(rsf_model)

```

##Gradient Boosting Machine

```{r}

# Fit Gradient Boosting Machine model
gbm_model <- gbm(Surv(fuyrs, status) ~ ., data = data, distribution = "coxph")

# Summary of the Gradient Boosting Machine model
print(gbm_model)

```

##Logistic Regression
```{r}
reg <- glm(status~.,data=data,family=binomial(link="logit"))
summary(reg)
```

```{r}
library(predtools)
library(magrittr)
library(dplyr)

data$pred <- predict.glm(reg, type = 'response')
#val_data$pred <- predict.glm(reg, newdata = val_data, type = 'response')
data$pred
calibration_plot(data = data, obs = "status", pred = "pred", title = "Calibration plot for development data", y_lim = c(0, 0.7), x_lim=c(0, 0.7))
#> $calibration_plot
```


##Models Comparison
```{r}

rsf_pred <- predict(rsf_model, data)$predicted
gbm_pred <- predict(gbm_model, data, type = "response")
# Calculate concordance index (C-index) for each model
cindex_rsf <- concordance(Surv(fuyrs, status) ~ rsf_pred, data)$concordance
cindex_gbm <- concordance(Surv(fuyrs, status) ~ gbm_pred, data)$concordance

# Print C-index for each model

cat("Random Survival Forest C-index:", cindex_rsf, "\n")
cat("Gradient Boosting Machine C-index:", cindex_gbm, "\n")

```


###Calibration plot
```{r}

#Risk quantiles of 1
q1 <- quantile(data$risk_base,  probs = seq(0.1, 0.9, 0.1))

# Risk classes of 1:
riskclass1 <- ifelse(data$risk_base < q1[1], 1,
                     ifelse(data$risk_base < q1[2], 2,
                            ifelse(data$risk_base < q1[3], 3,
                                   ifelse(data$risk_base < q1[4], 4,
                                          ifelse(data$risk_base < q1[5], 5,
                                                 ifelse(data$risk_base < q1[6], 6,
                                                        ifelse(data$risk_base < q1[7], 7,
                                                               ifelse(data$risk_base < q1[8], 8,
                                                                      ifelse(data$risk_base < q1[9], 9, 
                                                                             10)))))))))




# Observed proportion of events in risk classes of 1:
obsclass1 <- c()
for (i in 1:10) {
  group <- i
  ratio <- sum(data$status[riskclass1 == group]) / sum(riskclass1 == group)
  obsclass1 <- c(obsclass1, ratio)
}

# Average predicted risk in classes of 1:
meanriskclass1 <- c()
for (i in 1:10) {
  group <- i
  meanrisk <- mean(data$risk_base[riskclass1 == group])
  meanriskclass1 <- c(meanriskclass1, meanrisk)
}




#Risk quantiles of 2
q2 <- quantile(data$risk_augmented,  probs = seq(0.1, 0.9, 0.1))

# Risk classes of 1:
riskclass2 <- ifelse(data$risk_augmented < q2[1], 1,
                     ifelse(data$risk_augmented < q2[2], 2,
                            ifelse(data$risk_augmented < q2[3], 3,
                                   ifelse(data$risk_augmented < q2[4], 4,
                                          ifelse(data$risk_augmented < q2[5], 5,
                                                 ifelse(data$risk_augmented < q2[6], 6,
                                                        ifelse(data$risk_augmented < q2[7], 7,
                                                               ifelse(data$risk_augmented < q2[8], 8,
                                                                      ifelse(data$risk_augmented < q2[9], 9, 
                                                                             10)))))))))
# Observed proportion of events in risk classes of 2:
obsclass2 <- c()
for (i in 1:10) {
  group <- i
  ratio <- sum(data$status[riskclass2 == group]) / sum(riskclass2 == group)
  obsclass2 <- c(obsclass2, ratio)
}

# Average predicted risk in classes of 2:
meanriskclass2 <- c()
for (i in 1:10) {
  group <- i
  meanrisk <- mean(data$risk_augmented[riskclass2 == group])
  meanriskclass2 <- c(meanriskclass2, meanrisk)
}

# Calibration plot

par(mar=c(5,5,1,1))
plot(meanriskclass1, obsclass1, main = '', pch = 1, xlim = c(0,1), cex = 1.5,
     ylim = c(0,1), lwd = 3, ylab = 'Observed event rate', cex.lab = 1.7, cex.axis = 1.7,
     xlab = 'Average risk within decile', xaxt = "n", yaxt = "n", frame = F) 
axis(2, at = c(0,0.2,0.4,0.6,0.8,1), labels = NA, pos = 0)
axis(2, at = c(0,0.2,0.4,0.6,0.8,1), labels = c(0,0.2,0.4,0.6,0.8,1), cex.axis = 1.7, pos = 0)
axis(1, at = c(0,0.2,0.4,0.6,0.8,1), labels = c(0,0.2,0.4,0.6,0.8,1), cex.axis = 1.7, pos = 0)

points(meanriskclass2, obsclass2, lwd = 3, pch = 2, cex = 1.5)

lines(c(0, 1), c(0, 1), lty = 2, lwd = 2) 

lines(c(0,1), c(1,1), lty = 1)
lines(c(1,1), c(0,1), lty = 1)

legend(x = 0, y = 1, c("Base Model Risk","Augmented Model Risk"), lwd = c(3, 3), pch = c(1, 2), 
       bty = 'n', cex = 1.8, lty = c(NA,NA)) 

```
### Brier Score
```{r}
# indicator of death within 5 year
data$status.5y<-ifelse(data$fuyrs<=5,1,0)
# Brier Score
(BS_base <- mean((data$status.5y - data$risk_base) ^ 2)) 
(BS_augmented <- mean((data$status.5y - data$risk_augmented) ^ 2))
```
While the differences in Brier scores are small, the base model outperforms the augmented model in terms of predictive accuracy. This suggests that the additional variables included in the augmented model do not contribute positively to prediction and may even introduce noise, reducing overall performance.
```{r}
BS_basesc  <- mean(data$risk_base*(1-data$risk_base))
BS_augmentedsc <- mean(data$risk_augmented*(1-data$risk_augmented))

BS_base-BS_basesc
BS_augmented-BS_augmentedsc
```

###ROC AUC
```{r}
roc1<-roc(data$status.5y, data$risk_base)
plot(1 - roc1$specificities, roc1$sensitivities, 
     type = 'l', ylab = 'TPF', xlab = 'FPF', lwd = 3, xaxt = "n", yaxt = "n", 
     xlim = c(0,1), cex.lab = 1.7, frame = F)
axis(1, at = c(0,0.25,0.5,0.75,1), labels = NA, pos = 0)
axis(1, at = c(0,0.25,0.5,0.75,1), labels = c(0,0.25,0.5,0.75,1), cex.axis = 1.7, pos = 0)
axis(2, at = c(0,0.25,0.5,0.75,1), labels = c(0,0.25,0.5,0.75,1), cex.axis = 1.7, pos = 0)
Youden1<-roc1$sensitivities+roc1$specificities-1
optimal.cut.off1<-roc1$thresholds[Youden1==max(Youden1)]
cbind(optimal.cut.off1,Youden=max(Youden1))
points(1-roc1$specificities[roc1$thresholds==optimal.cut.off1],roc1$sensitivities[roc1$thresholds==optimal.cut.off1],pch=0,cex=1.7)

roc2<-roc(data$status.5y, data$risk_augmented)

lines(1 - roc2$specificities, roc2$sensitivities, 
      lwd = 3, lty = 3)
Youden2<-roc2$sensitivities+roc2$specificities-1
optimal.cut.off2<-roc2$thresholds[Youden2==max(Youden2)]
cbind(optimal.cut.off2,Youden=max(Youden2))


points(1-roc2$specificities[roc2$thresholds==optimal.cut.off2],roc2$sensitivities[roc2$thresholds==optimal.cut.off2],pch=0,cex=1.7)

# Chance line:
abline(a=0, b=1, lty = 2, lwd = 2)
lines(c(0,1), c(1,1), lty = 1)
lines(c(1,1), c(0,1), lty = 1)

legend(x = 0, y = 1, c("model1","model2"), lwd = c(3,3), lty = c(1,3), bty = 'n', cex = 1.7)


```




```{r}
(AUC1 <- roc1$auc)
(AUC2 <- roc2$auc)   
AUC2 - AUC1
```


```{r}

score<- Score(list("model1"=base_model_1,"model2"=augmented_model_2),
              formula=Surv(fuyrs, status==1)~1,
              data,conf.int=T,
              times=seq(1,5,1),
              plots=c("calibration","ROC"))

plotROC(score,times=5,cens.method="local")
title(main="time-dependent ROC at 1 years")
```
```{r}
plotCalibration(score,times=5,cens.method="local",method="quantile",q=10)
title(main="calibration at 1 years")

```

```{r}

##NOT WORKING
library(survival)
library(riskRegression)

base_model_1 <- coxph(Surv(fuyrs, status) ~ sex + age + con.cabg + creat + lv + sten.reg.mix, data, x = TRUE)

augmented_model_2 <- coxph(Surv(fuyrs, status) ~ sex + age + con.cabg + creat + lv + sten.reg.mix + log.lvmi, data, x = TRUE)

# internal validation using bootstrap
CVscore<- Score(list("model1"=base_model_1,"model2"=augmented_model_2),
                formula=Surv(fuyrs, status==1)~1,
                data,conf.int=T,
                times=seq(1,5,1),
                split.method="loob",B=200,seed=1803)

CVscore

data
```


###NET BENEFIT

```{r}
library(dcurves)
dca(Surv(fuyrs, status==1) ~ risk_base + risk_augmented, data, time = 5)
```

##ALEX TEST

```{r}
# Function to run univariate Cox model and return summary
run_cox_model <- function(variable) {
  formula <- as.formula(paste("Surv(fuyrs, status) ~", variable))
  model <- coxph(formula, data = data)
  summary(model)
}

# List of variables to test
variables <- c("log.lvmi", "sex", "age", "con.cabg", "creat", "lv", "sten.reg.mix")

# Apply the function to each variable and store results
model_results <- lapply(variables, run_cox_model)

# Print the results
model_results



```

#REDUCED RANK REGRESSION (TEST)

```{r}
library(survival)

# Define the reduced rank structure matrix
# Here, let's say we want to reduce the rank to 2
# So, we have a 7x2 structure matrix
structure_matrix <- matrix(0, nrow = 7, ncol = 2)

# Filling the structure matrix with appropriate coefficients
# For simplicity, let's assume all coefficients are 1
structure_matrix[] <- 1

# Fit the Cox proportional hazards model with reduced rank
reduced_rank_model <- coxph(Surv(fuyrs, status) ~ sex + age + con.cabg + creat + lv + sten.reg.mix + log.lvmi +
                               tt(sex) + tt(age) + tt(con.cabg) + tt(creat) + tt(lv) + tt(sten.reg.mix) + tt(log.lvmi),
                            tt = structure_matrix,
                            data = data)

# Summary of the reduced rank model
summary(reduced_rank_model)

# Compare the old model with the new one
print("Old Model:")
summary(augmented_model)
print("Reduced Rank Model:")
summary(reduced_rank_model)


```

